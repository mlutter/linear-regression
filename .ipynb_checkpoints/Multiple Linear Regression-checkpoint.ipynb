{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "### Here with a train / test split, taking all the predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.7109203586326271\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "reg_all = linear_model.LinearRegression()\n",
    "reg_all.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg_all.predict(X_test)\n",
    "\n",
    "print(\"R-squared: {}\".format(reg_all.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here with cross validation instead of a train/test split. This helps control for selection bias by doing the splitting into some number of \"folds\" that get train/tested against eachother, scores result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.63861069  0.71334432  0.58645134  0.07842495 -0.26312455]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "cv_results = cross_val_score(reg, X, y, cv=5) #get the five-fold cross validation\n",
    "\n",
    "print(cv_results) #prints the R-squared for each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model - backward elimination\n",
    "\n",
    "The following code came from KIRILL EREMENKO machine learning course https://www.superdatascience.com/machine-learning/ This is used to eliminate some less useful features as determined by the r-squared and p-values by backward elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.741\n",
      "Model:                            OLS   Adj. R-squared:                  0.735\n",
      "Method:                 Least Squares   F-statistic:                     128.2\n",
      "Date:                Fri, 06 Apr 2018   Prob (F-statistic):          5.74e-137\n",
      "Time:                        14:50:13   Log-Likelihood:                -1498.9\n",
      "No. Observations:                 506   AIC:                             3022.\n",
      "Df Residuals:                     494   BIC:                             3073.\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         18.1847      2.534      7.176      0.000      13.205      23.164\n",
      "x1            18.1847      2.534      7.176      0.000      13.205      23.164\n",
      "x2            -0.1076      0.033     -3.296      0.001      -0.172      -0.043\n",
      "x3             0.0458      0.014      3.387      0.001       0.019       0.072\n",
      "x4             2.7212      0.854      3.185      0.002       1.043       4.400\n",
      "x5           -17.3956      3.536     -4.920      0.000     -24.343     -10.448\n",
      "x6             3.7966      0.406      9.343      0.000       2.998       4.595\n",
      "x7            -1.4934      0.186     -8.039      0.000      -1.858      -1.128\n",
      "x8             0.2991      0.063      4.719      0.000       0.175       0.424\n",
      "x9            -0.0118      0.003     -3.488      0.001      -0.018      -0.005\n",
      "x10           -0.9471      0.129     -7.337      0.000      -1.201      -0.693\n",
      "x11            0.0094      0.003      3.508      0.000       0.004       0.015\n",
      "x12           -0.5232      0.047    -11.037      0.000      -0.616      -0.430\n",
      "==============================================================================\n",
      "Omnibus:                      178.444   Durbin-Watson:                   1.078\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              786.944\n",
      "Skew:                           1.524   Prob(JB):                    1.31e-171\n",
      "Kurtosis:                       8.295   Cond. No.                     4.07e+16\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 9.42e-26. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "def backwardElimination(x, SL):\n",
    "    numVars = len(x[0])\n",
    "    rows = len(x)\n",
    "    temp = np.zeros((rows,numVars)).astype(int)\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n",
    "        if maxVar > SL:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    temp[:,j] = x[:, j]\n",
    "                    x = np.delete(x, j, 1)\n",
    "                    tmp_regressor = sm.OLS(y, x).fit()\n",
    "                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n",
    "                    if (adjR_before >= adjR_after):\n",
    "                        x_rollback = np.hstack((x, temp[:,[0,j]]))\n",
    "                        x_rollback = np.delete(x_rollback, j, 1)\n",
    "                        print (regressor_OLS.summary())\n",
    "                        return x_rollback\n",
    "                    else:\n",
    "                        continue\n",
    "    print(regressor_OLS.summary())\n",
    "    return x\n",
    " \n",
    "SL = 0.05\n",
    "#this needs a column of ones on the front to work right.\n",
    "X = np.append(arr = np.ones((len(X), 1)).astype(int), values = X, axis = 1)\n",
    "X_Modeled = backwardElimination(X, SL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datacamp]",
   "language": "python",
   "name": "conda-env-datacamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
